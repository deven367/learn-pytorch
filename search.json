[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "learn-pytorch",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "learn-pytorch",
    "section": "Install",
    "text": "Install\npip install learn_pytorch"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "learn-pytorch",
    "section": "How to use",
    "text": "How to use\nFill me in please! Donâ€™t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "02-nn-classification.html",
    "href": "02-nn-classification.html",
    "title": "nn classification",
    "section": "",
    "text": "from sklearn.datasets import make_circles\nimport matplotlib.pyplot as plt\n\n# Make 1000 samples\nn_samples = 1000\n\n# Create circles\nX, y = make_circles(n_samples,\n                    noise=0.03, # a little bit of noise to the dots\n                    random_state=42) # keep random state so we get the same values\n\n\nplt.scatter(x=X[:, 0],\n            y=X[:, 1],\n            c=y,\n            cmap=plt.cm.RdYlBu);\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nX_train.shape\n\ntorch.Size([800, 2])\n\n\n\nclass CircleClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Linear(2, 10)\n        self.l2 = nn.Linear(10, 1)\n    def forward(self, x):\n        return self.l2(self.l1(x))\n\n\nclass CircleClassifierImproved(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Linear(2, 10)\n        self.l2 = nn.Linear(10, 10)\n        self.l3 = nn.Linear(10, 1)\n        self.relu = nn.ReLU()\n    def forward(self, x):\n        return self.l3(self.relu(self.l2(self.relu(self.l1(x)))))\n\n\nt = torch.from_numpy(X_train.numpy()).type(torch.float32)\n\n\nt\n\ntensor([[ 0.6579, -0.4651],\n        [ 0.6319, -0.7347],\n        [-1.0086, -0.1240],\n        ...,\n        [ 0.0157, -1.0300],\n        [ 1.0110,  0.1680],\n        [ 0.5578, -0.5709]])\n\n\nHalf is float16, Float is float32\n\nf = nn.Linear(2, 10)(t)\n\n\nnn.Linear(2, 10)(t).shape\n\ntorch.Size([800, 10])\n\n\n\n# c = CircleClassifier()\nc = CircleClassifierImproved()\n\n\nc.parameters\n\n&lt;bound method Module.parameters of CircleClassifierImproved(\n  (l1): Linear(in_features=2, out_features=10, bias=True)\n  (l2): Linear(in_features=10, out_features=10, bias=True)\n  (l3): Linear(in_features=10, out_features=1, bias=True)\n  (relu): ReLU()\n)&gt;\n\n\n\nloss_fn = nn.BCEWithLogitsLoss()\nopt = torch.optim.Adam(c.parameters(), lr=0.01)\n\n\ntr_loss = []\nvl_loss = []\n\n\n# Calculate accuracy (a classification metric)\ndef accuracy_fn(y_true, y_pred):\n    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n    acc = (correct / len(y_pred)) * 100\n    return acc\n\n\n# training loop\nfor epoch in range(100):\n    c.train()\n\n    y_pred = c(X_train).squeeze()\n    act_pred = torch.sigmoid(y_pred).round()\n    loss = loss_fn(y_pred, y_train)\n\n    tr_loss.append(loss.detach().numpy())\n    acc = accuracy_fn(y_train, act_pred)\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n\n    c.eval()\n    with torch.inference_mode():\n        v_pred = c(X_test).squeeze()\n        act_pred = torch.sigmoid(v_pred).round()\n        vloss = loss_fn(v_pred, y_test)\n        vl_loss.append(vloss.detach().numpy())\n        test_acc = accuracy_fn(y_test, act_pred)\n    print(f\"Epoch: {epoch+1:&gt;5} | Loss: {tr_loss[epoch]:.5f}, Accuracy: {acc:.2f}% | Test loss: {vl_loss[epoch]:.5f}, Test acc: {test_acc:.2f}%\")\n\nEpoch:     1 | Loss: 0.69395, Accuracy: 49.25% | Test loss: 0.69459, Test acc: 48.00%\nEpoch:     2 | Loss: 0.69282, Accuracy: 50.00% | Test loss: 0.69388, Test acc: 52.50%\nEpoch:     3 | Loss: 0.69172, Accuracy: 51.38% | Test loss: 0.69347, Test acc: 53.50%\nEpoch:     4 | Loss: 0.69103, Accuracy: 54.62% | Test loss: 0.69311, Test acc: 48.00%\nEpoch:     5 | Loss: 0.69049, Accuracy: 52.25% | Test loss: 0.69285, Test acc: 48.50%\nEpoch:     6 | Loss: 0.68994, Accuracy: 53.50% | Test loss: 0.69273, Test acc: 47.00%\nEpoch:     7 | Loss: 0.68938, Accuracy: 53.12% | Test loss: 0.69262, Test acc: 47.50%\nEpoch:     8 | Loss: 0.68879, Accuracy: 53.62% | Test loss: 0.69252, Test acc: 47.00%\nEpoch:     9 | Loss: 0.68816, Accuracy: 53.50% | Test loss: 0.69239, Test acc: 47.50%\nEpoch:    10 | Loss: 0.68747, Accuracy: 53.62% | Test loss: 0.69222, Test acc: 48.00%\nEpoch:    11 | Loss: 0.68673, Accuracy: 53.75% | Test loss: 0.69196, Test acc: 47.50%\nEpoch:    12 | Loss: 0.68592, Accuracy: 53.62% | Test loss: 0.69162, Test acc: 48.00%\nEpoch:    13 | Loss: 0.68504, Accuracy: 53.12% | Test loss: 0.69120, Test acc: 48.00%\nEpoch:    14 | Loss: 0.68407, Accuracy: 53.25% | Test loss: 0.69071, Test acc: 48.00%\nEpoch:    15 | Loss: 0.68302, Accuracy: 53.37% | Test loss: 0.69010, Test acc: 48.00%\nEpoch:    16 | Loss: 0.68186, Accuracy: 53.62% | Test loss: 0.68933, Test acc: 48.00%\nEpoch:    17 | Loss: 0.68050, Accuracy: 53.75% | Test loss: 0.68833, Test acc: 47.50%\nEpoch:    18 | Loss: 0.67886, Accuracy: 53.87% | Test loss: 0.68703, Test acc: 48.50%\nEpoch:    19 | Loss: 0.67695, Accuracy: 53.75% | Test loss: 0.68569, Test acc: 48.50%\nEpoch:    20 | Loss: 0.67495, Accuracy: 54.00% | Test loss: 0.68422, Test acc: 48.50%\nEpoch:    21 | Loss: 0.67285, Accuracy: 54.00% | Test loss: 0.68263, Test acc: 49.50%\nEpoch:    22 | Loss: 0.67058, Accuracy: 54.62% | Test loss: 0.68087, Test acc: 49.50%\nEpoch:    23 | Loss: 0.66812, Accuracy: 54.87% | Test loss: 0.67885, Test acc: 49.50%\nEpoch:    24 | Loss: 0.66548, Accuracy: 55.12% | Test loss: 0.67641, Test acc: 49.50%\nEpoch:    25 | Loss: 0.66254, Accuracy: 55.38% | Test loss: 0.67345, Test acc: 50.50%\nEpoch:    26 | Loss: 0.65915, Accuracy: 55.50% | Test loss: 0.67005, Test acc: 51.00%\nEpoch:    27 | Loss: 0.65543, Accuracy: 56.38% | Test loss: 0.66632, Test acc: 54.00%\nEpoch:    28 | Loss: 0.65144, Accuracy: 57.75% | Test loss: 0.66228, Test acc: 54.50%\nEpoch:    29 | Loss: 0.64706, Accuracy: 62.12% | Test loss: 0.65776, Test acc: 64.00%\nEpoch:    30 | Loss: 0.64221, Accuracy: 68.38% | Test loss: 0.65275, Test acc: 70.00%\nEpoch:    31 | Loss: 0.63695, Accuracy: 72.62% | Test loss: 0.64736, Test acc: 73.00%\nEpoch:    32 | Loss: 0.63136, Accuracy: 78.38% | Test loss: 0.64178, Test acc: 75.50%\nEpoch:    33 | Loss: 0.62557, Accuracy: 82.50% | Test loss: 0.63626, Test acc: 81.50%\nEpoch:    34 | Loss: 0.61948, Accuracy: 85.62% | Test loss: 0.63057, Test acc: 85.00%\nEpoch:    35 | Loss: 0.61304, Accuracy: 88.12% | Test loss: 0.62463, Test acc: 85.50%\nEpoch:    36 | Loss: 0.60619, Accuracy: 89.25% | Test loss: 0.61840, Test acc: 87.00%\nEpoch:    37 | Loss: 0.59899, Accuracy: 91.00% | Test loss: 0.61182, Test acc: 87.00%\nEpoch:    38 | Loss: 0.59143, Accuracy: 91.88% | Test loss: 0.60494, Test acc: 87.00%\nEpoch:    39 | Loss: 0.58345, Accuracy: 92.50% | Test loss: 0.59751, Test acc: 87.50%\nEpoch:    40 | Loss: 0.57512, Accuracy: 93.12% | Test loss: 0.58961, Test acc: 90.00%\nEpoch:    41 | Loss: 0.56640, Accuracy: 93.50% | Test loss: 0.58136, Test acc: 90.50%\nEpoch:    42 | Loss: 0.55733, Accuracy: 94.50% | Test loss: 0.57267, Test acc: 91.00%\nEpoch:    43 | Loss: 0.54794, Accuracy: 95.88% | Test loss: 0.56366, Test acc: 92.50%\nEpoch:    44 | Loss: 0.53824, Accuracy: 96.12% | Test loss: 0.55430, Test acc: 92.00%\nEpoch:    45 | Loss: 0.52829, Accuracy: 96.25% | Test loss: 0.54468, Test acc: 91.50%\nEpoch:    46 | Loss: 0.51800, Accuracy: 96.75% | Test loss: 0.53484, Test acc: 91.50%\nEpoch:    47 | Loss: 0.50735, Accuracy: 97.00% | Test loss: 0.52465, Test acc: 93.50%\nEpoch:    48 | Loss: 0.49646, Accuracy: 97.50% | Test loss: 0.51404, Test acc: 95.00%\nEpoch:    49 | Loss: 0.48529, Accuracy: 97.88% | Test loss: 0.50296, Test acc: 95.50%\nEpoch:    50 | Loss: 0.47391, Accuracy: 98.12% | Test loss: 0.49164, Test acc: 95.50%\nEpoch:    51 | Loss: 0.46230, Accuracy: 98.00% | Test loss: 0.48024, Test acc: 95.50%\nEpoch:    52 | Loss: 0.45044, Accuracy: 97.88% | Test loss: 0.46885, Test acc: 93.50%\nEpoch:    53 | Loss: 0.43832, Accuracy: 98.00% | Test loss: 0.45750, Test acc: 93.50%\nEpoch:    54 | Loss: 0.42590, Accuracy: 98.12% | Test loss: 0.44618, Test acc: 93.50%\nEpoch:    55 | Loss: 0.41338, Accuracy: 98.12% | Test loss: 0.43490, Test acc: 94.00%\nEpoch:    56 | Loss: 0.40085, Accuracy: 98.38% | Test loss: 0.42354, Test acc: 94.00%\nEpoch:    57 | Loss: 0.38824, Accuracy: 98.62% | Test loss: 0.41168, Test acc: 93.50%\nEpoch:    58 | Loss: 0.37554, Accuracy: 98.75% | Test loss: 0.39968, Test acc: 93.50%\nEpoch:    59 | Loss: 0.36280, Accuracy: 98.75% | Test loss: 0.38801, Test acc: 94.50%\nEpoch:    60 | Loss: 0.35000, Accuracy: 99.00% | Test loss: 0.37668, Test acc: 95.50%\nEpoch:    61 | Loss: 0.33730, Accuracy: 99.12% | Test loss: 0.36468, Test acc: 94.50%\nEpoch:    62 | Loss: 0.32471, Accuracy: 99.12% | Test loss: 0.35273, Test acc: 95.50%\nEpoch:    63 | Loss: 0.31209, Accuracy: 99.25% | Test loss: 0.34076, Test acc: 96.00%\nEpoch:    64 | Loss: 0.29957, Accuracy: 99.25% | Test loss: 0.32855, Test acc: 96.50%\nEpoch:    65 | Loss: 0.28718, Accuracy: 99.38% | Test loss: 0.31671, Test acc: 97.50%\nEpoch:    66 | Loss: 0.27486, Accuracy: 99.25% | Test loss: 0.30487, Test acc: 97.50%\nEpoch:    67 | Loss: 0.26281, Accuracy: 99.38% | Test loss: 0.29303, Test acc: 98.50%\nEpoch:    68 | Loss: 0.25092, Accuracy: 99.38% | Test loss: 0.28186, Test acc: 98.50%\nEpoch:    69 | Loss: 0.23924, Accuracy: 99.88% | Test loss: 0.27086, Test acc: 98.00%\nEpoch:    70 | Loss: 0.22795, Accuracy: 99.88% | Test loss: 0.25965, Test acc: 98.00%\nEpoch:    71 | Loss: 0.21713, Accuracy: 99.75% | Test loss: 0.24917, Test acc: 98.50%\nEpoch:    72 | Loss: 0.20682, Accuracy: 99.75% | Test loss: 0.23911, Test acc: 99.00%\nEpoch:    73 | Loss: 0.19695, Accuracy: 99.75% | Test loss: 0.22970, Test acc: 99.00%\nEpoch:    74 | Loss: 0.18754, Accuracy: 99.75% | Test loss: 0.22104, Test acc: 99.50%\nEpoch:    75 | Loss: 0.17861, Accuracy: 99.75% | Test loss: 0.21228, Test acc: 99.50%\nEpoch:    76 | Loss: 0.17008, Accuracy: 99.75% | Test loss: 0.20408, Test acc: 99.50%\nEpoch:    77 | Loss: 0.16195, Accuracy: 99.75% | Test loss: 0.19655, Test acc: 99.50%\nEpoch:    78 | Loss: 0.15414, Accuracy: 99.75% | Test loss: 0.18935, Test acc: 99.50%\nEpoch:    79 | Loss: 0.14669, Accuracy: 99.75% | Test loss: 0.18272, Test acc: 99.50%\nEpoch:    80 | Loss: 0.13957, Accuracy: 99.75% | Test loss: 0.17621, Test acc: 99.50%\nEpoch:    81 | Loss: 0.13279, Accuracy: 99.75% | Test loss: 0.16968, Test acc: 99.50%\nEpoch:    82 | Loss: 0.12630, Accuracy: 99.75% | Test loss: 0.16349, Test acc: 99.50%\nEpoch:    83 | Loss: 0.12013, Accuracy: 99.75% | Test loss: 0.15734, Test acc: 99.50%\nEpoch:    84 | Loss: 0.11429, Accuracy: 99.75% | Test loss: 0.15152, Test acc: 99.50%\nEpoch:    85 | Loss: 0.10875, Accuracy: 99.88% | Test loss: 0.14605, Test acc: 99.50%\nEpoch:    86 | Loss: 0.10347, Accuracy: 99.88% | Test loss: 0.14066, Test acc: 100.00%\nEpoch:    87 | Loss: 0.09847, Accuracy: 99.88% | Test loss: 0.13540, Test acc: 100.00%\nEpoch:    88 | Loss: 0.09374, Accuracy: 99.88% | Test loss: 0.13038, Test acc: 100.00%\nEpoch:    89 | Loss: 0.08927, Accuracy: 99.88% | Test loss: 0.12535, Test acc: 100.00%\nEpoch:    90 | Loss: 0.08506, Accuracy: 99.88% | Test loss: 0.12052, Test acc: 100.00%\nEpoch:    91 | Loss: 0.08111, Accuracy: 99.88% | Test loss: 0.11580, Test acc: 99.50%\nEpoch:    92 | Loss: 0.07738, Accuracy: 100.00% | Test loss: 0.11105, Test acc: 99.50%\nEpoch:    93 | Loss: 0.07385, Accuracy: 100.00% | Test loss: 0.10668, Test acc: 99.50%\nEpoch:    94 | Loss: 0.07050, Accuracy: 100.00% | Test loss: 0.10263, Test acc: 99.50%\nEpoch:    95 | Loss: 0.06736, Accuracy: 100.00% | Test loss: 0.09872, Test acc: 99.50%\nEpoch:    96 | Loss: 0.06441, Accuracy: 100.00% | Test loss: 0.09504, Test acc: 99.50%\nEpoch:    97 | Loss: 0.06163, Accuracy: 100.00% | Test loss: 0.09157, Test acc: 99.50%\nEpoch:    98 | Loss: 0.05901, Accuracy: 100.00% | Test loss: 0.08830, Test acc: 99.50%\nEpoch:    99 | Loss: 0.05656, Accuracy: 100.00% | Test loss: 0.08528, Test acc: 100.00%\nEpoch:   100 | Loss: 0.05425, Accuracy: 100.00% | Test loss: 0.08247, Test acc: 100.00%\n\n\n\nplt.plot(tr_loss)\nplt.plot(vl_loss);"
  },
  {
    "objectID": "00-fundamentals.html",
    "href": "00-fundamentals.html",
    "title": "fundamentals",
    "section": "",
    "text": "!which python\n\n/Users/deven367/mambaforge/envs/pth/bin/python\n!pip show torch\n\nName: torch\nVersion: 2.0.1\nSummary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\nHome-page: https://pytorch.org/\nAuthor: PyTorch Team\nAuthor-email: packages@pytorch.org\nLicense: BSD-3\nLocation: /Users/deven367/mambaforge/envs/pth/lib/python3.10/site-packages\nRequires: filelock, jinja2, networkx, sympy, typing-extensions\nRequired-by: accelerate, fastai, peft, torchaudio, torchvision\nimport torch\ntorch.__version__\n\n'2.0.1'"
  },
  {
    "objectID": "00-fundamentals.html#scalar",
    "href": "00-fundamentals.html#scalar",
    "title": "fundamentals",
    "section": "scalar",
    "text": "scalar\n\n# Scalar\nscalar = torch.tensor(7)\nscalar\n\ntensor(7)\n\n\n\nscalar.item()\n\n7\n\n\n\nscalar.ndim\n\n0"
  },
  {
    "objectID": "00-fundamentals.html#vector",
    "href": "00-fundamentals.html#vector",
    "title": "fundamentals",
    "section": "vector",
    "text": "vector\n\nvector = torch.tensor([1, 2])\nvector\n\ntensor([1, 2])"
  },
  {
    "objectID": "00-fundamentals.html#matrix",
    "href": "00-fundamentals.html#matrix",
    "title": "fundamentals",
    "section": "matrix",
    "text": "matrix\n\n# Matrix\nMATRIX = torch.tensor([[7, 8],\n                       [9, 10]])\nMATRIX\n\ntensor([[ 7,  8],\n        [ 9, 10]])\n\n\n\nMATRIX.ndim\n\n2"
  },
  {
    "objectID": "00-fundamentals.html#tensor",
    "href": "00-fundamentals.html#tensor",
    "title": "fundamentals",
    "section": "tensor",
    "text": "tensor\n\n# Tensor\nTENSOR = torch.tensor([[[1, 2, 3],\n                        [3, 6, 9],\n                        [2, 4, 5]]])\nTENSOR\n\ntensor([[[1, 2, 3],\n         [3, 6, 9],\n         [2, 4, 5]]])\n\n\n\nTENSOR.shape\n\ntorch.Size([1, 3, 3])"
  },
  {
    "objectID": "00-fundamentals.html#random-tensor",
    "href": "00-fundamentals.html#random-tensor",
    "title": "fundamentals",
    "section": "random tensor",
    "text": "random tensor\n\nx = torch.rand(size=(3,4))\n\n\nx, x.dtype\n\n(tensor([[0.0054, 0.3713, 0.6901, 0.3464],\n         [0.7347, 0.2866, 0.8748, 0.8203],\n         [0.8466, 0.4066, 0.3717, 0.1785]]),\n torch.float32)\n\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.imshow(x)\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\n\nx3 = torch.rand(size=(3, 500, 500))\nx3\n\ntensor([[[0.3124, 0.2393, 0.6851,  ..., 0.5956, 0.0522, 0.9978],\n         [0.4588, 0.5240, 0.2963,  ..., 0.2372, 0.2270, 0.9578],\n         [0.6997, 0.8471, 0.7639,  ..., 0.1464, 0.2590, 0.8607],\n         ...,\n         [0.6741, 0.5199, 0.0177,  ..., 0.9062, 0.1984, 0.2768],\n         [0.8433, 0.9803, 0.4615,  ..., 0.7602, 0.1122, 0.0832],\n         [0.9944, 0.1375, 0.7048,  ..., 0.8367, 0.4785, 0.7167]],\n\n        [[0.3095, 0.4969, 0.4572,  ..., 0.4483, 0.9172, 0.3245],\n         [0.6805, 0.3055, 0.0011,  ..., 0.0420, 0.0053, 0.6516],\n         [0.2692, 0.3799, 0.4305,  ..., 0.9412, 0.0964, 0.4699],\n         ...,\n         [0.8962, 0.6808, 0.7867,  ..., 0.7209, 0.0227, 0.6804],\n         [0.8378, 0.6278, 0.9680,  ..., 0.2193, 0.5567, 0.7397],\n         [0.6402, 0.2966, 0.7525,  ..., 0.3614, 0.1355, 0.9917]],\n\n        [[0.5723, 0.2855, 0.3038,  ..., 0.4785, 0.7347, 0.5672],\n         [0.3403, 0.0803, 0.9781,  ..., 0.9861, 0.6144, 0.2424],\n         [0.1284, 0.1958, 0.3418,  ..., 0.3563, 0.6601, 0.2327],\n         ...,\n         [0.2266, 0.1774, 0.5236,  ..., 0.0964, 0.6774, 0.3021],\n         [0.5507, 0.0731, 0.5770,  ..., 0.0038, 0.2636, 0.4521],\n         [0.0790, 0.1722, 0.8542,  ..., 0.2528, 0.7655, 0.9268]]])\n\n\n\nx3.T.shape\n\n/var/folders/83/lj3mdwvd2s12wl1q96t6lkvr0000gn/T/ipykernel_7615/3902085171.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343686130/work/aten/src/ATen/native/TensorShape.cpp:3575.)\n  x3.T.shape\n\n\ntorch.Size([500, 500, 3])\n\n\n\nplt.imshow(x3.T);"
  },
  {
    "objectID": "00-fundamentals.html#tensor-manipulation",
    "href": "00-fundamentals.html#tensor-manipulation",
    "title": "fundamentals",
    "section": "tensor manipulation",
    "text": "tensor manipulation\n\nTENSOR, TENSOR.dtype\n\n(tensor([[[1, 2, 3],\n          [3, 6, 9],\n          [2, 4, 5]]]),\n torch.int64)\n\n\n\nTENSOR + 10\n\ntensor([[[11, 12, 13],\n         [13, 16, 19],\n         [12, 14, 15]]])\n\n\n\nTENSOR * 10\n\ntensor([[[10, 20, 30],\n         [30, 60, 90],\n         [20, 40, 50]]])\n\n\n\ntf = torch.tensor([[[1, 2, 3],\n                    [3, 6, 9],\n                    [2, 4, 5]]], dtype = torch.float32)\n\n\nTENSOR * tf\n\ntensor([[[ 1.,  4.,  9.],\n         [ 9., 36., 81.],\n         [ 4., 16., 25.]]])"
  },
  {
    "objectID": "00-fundamentals.html#linear-layer",
    "href": "00-fundamentals.html#linear-layer",
    "title": "fundamentals",
    "section": "linear layer",
    "text": "linear layer\n\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator&gt;\n\n\n\nl1 = torch.nn.Linear(2, 10)\n\n\nl1(torch.tensor([1., 1.]))\n\ntensor([ 0.8017,  0.4010, -0.2994,  0.5401, -0.4536,  0.4210,  0.4185, -0.1840,\n         0.7164, -0.8483], grad_fn=&lt;AddBackward0&gt;)\n\n\n\ntt = torch.tensor([[[1., 2.]]])\ntt, tt.shape\n\n(tensor([[[1., 2.]]]), torch.Size([1, 1, 2]))\n\n\n\ntorch.squeeze(tt), torch.squeeze(tt).shape\n\n(tensor([1., 2.]), torch.Size([2]))"
  },
  {
    "objectID": "00-fundamentals.html#cuda-check",
    "href": "00-fundamentals.html#cuda-check",
    "title": "fundamentals",
    "section": "cuda check",
    "text": "cuda check\n\ntorch.cuda.is_available()\n\nFalse"
  },
  {
    "objectID": "micrograd.html",
    "href": "micrograd.html",
    "title": "micrograd",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\ndef f(x):\n    return 3*x**2 - 4*x + 5\n\n\nf(3.)\n\n20.0\n\n\n\nfrom graphviz import Digraph\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n\n    for n in nodes:\n        uid = str(id(n))\n        dot.node(name=uid, label = \"{ %s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad), shape='record')\n        if n._op:\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))\n\n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        \n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        \n        out._backward = _backward\n            \n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        \n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        \n        out._backward = _backward\n        \n        return out\n\n    def __rmul__(self, other): # other * self\n        return self * other\n\n    def __truediv__(self, other): # self / other\n        return self * other**-1\n\n    def __neg__(self): # -self\n        return self * -1\n\n    def __sub__(self, other): # self - other\n        return self + (-other)\n\n    def __radd__(self, other): # other + self\n        return self + other\n    \n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self, ), f'**other')\n        \n        def _backward():\n            self.grad += (other * out.data**(other-1)) * out.grad\n        \n        out._backward = _backward\n        return out\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self, ), 'tanh')\n        \n        def _backward():\n            self.grad += (1 - t**2) * out.grad\n            \n        out._backward = _backward\n        return out\n    \n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self, ), 'exp')\n        \n        def _backward():\n            self.grad += out.data * out.grad\n        \n        out._backward = _backward\n        return out\n    \n    def backward(self):\n    \n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n\na = Value(2., label = 'a')\nb = Value(-3., label = 'b')\nc = Value(10., label = 'c')\ne = a*b; e.label = 'e'\nd = e + c; d.label = 'd'\nf = Value(-2., label='f')\nL = d * f; L.label = 'L'\nL\n\nValue(data=-8.0)\n\n\n\nValue(1) +  2\n\nValue(data=3)\n\n\n\ndef lol():\n\n    h = 0.001\n\n    a = Value(2.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(10.0, label='c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L1 = L.data\n\n    a = Value(2.0, label='a')\n    b = Value(-3.0, label='b')\n    b.data += h\n    c = Value(10.0, label='c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L2 = L.data\n\n    print((L2 - L1)/h)\n\nlol()\n\n-3.9999999999995595\n\n\n\nd._prev, d._op\n\n({Value(data=-6.0), Value(data=10.0)}, '+')\n\n\n\nL.grad = 1.\nf.grad = 4.\nd.grad = -2\n\n\nc.grad = -2.\ne.grad = -2.\n\n\na.grad = 6.\nb.grad = -4.\n\n\ndraw_dot(L)\n\n\n\n\n\nplt.plot(np.arange(-5,5,0.2), np.tanh(np.arange(-5,5,0.2))); plt.grid();\n\n\n\n\n\n# inputs x1,x2\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\n# weights w1,w2\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\n# bias of the neuron\nb = Value(6.8813735870195432, label='b')\n\n# x1*w1 + x2*w2 + b\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n\nn = x1w1x2w2 + b; n.label = 'n'\no = n.tanh(); o.label = 'o'\n\n\ndraw_dot(o)\n\n\n\n\n\no.backward()\n\n\na = Value(3.0, label='b')\nb = a + a; b.label = 'b'\nb.backward()\ndraw_dot(b)\n\n\n\n\n\n# inputs x1,x2\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n# weights w1,w2\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n# bias of the neuron\nb = Value(6.8813735870195432, label='b')\n# x1*w1 + x2*w2 + b\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\nn = x1w1x2w2 + b; n.label = 'n'\n# ----\ne = (2*n).exp()\no = (e - 1) / (e + 1)\n# ----\no.label = 'o'\no.backward()\ndraw_dot(o)\n\n\n\n\n\n(2*n).exp()\n\nValue(data=5.828427124746192)\n\n\n\nValue(10)**2\n\nValue(data=100)\n\n\n\nimport torch\n\n\nx1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\nx2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\nw1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\nw2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\nb = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\nn = x1*w1 + x2*w2 + b\no = torch.tanh(n)\n\nprint(o.data.item())\no.backward()\n\nprint('---')\nprint('x2', x2.grad.item())\nprint('w2', w2.grad.item())\nprint('x1', x1.grad.item())\nprint('w1', w1.grad.item())\n\n0.7071066904050358\n---\nx2 0.5000001283844369\nw2 0.0\nx1 -1.5000003851533106\nw1 1.0000002567688737\n\n\n\no.data\n\ntensor([0.7071], dtype=torch.float64)\n\n\n\nimport random\n\n\nclass Neuron:\n  \n  def __init__(self, nin):\n    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n    self.b = Value(random.uniform(-1,1))\n  \n  def __call__(self, x):\n    # w * x + b\n    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n    out = act.tanh()\n    return out\n  \n  def parameters(self):\n    return self.w + [self.b]\n\nclass Layer:\n  \n  def __init__(self, nin, nout):\n    self.neurons = [Neuron(nin) for _ in range(nout)]\n  \n  def __call__(self, x):\n    outs = [n(x) for n in self.neurons]\n    return outs[0] if len(outs) == 1 else outs\n  \n  def parameters(self):\n    return [p for neuron in self.neurons for p in neuron.parameters()]\n\nclass MLP:\n  \n  def __init__(self, nin, nouts):\n    sz = [nin] + nouts\n    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n  \n  def __call__(self, x):\n    for layer in self.layers:\n      x = layer(x)\n    return x\n  def parameters(self):\n    return [p for layer in self.layers for p in layer.parameters()]\n\n\nx = [2.0, 3.0, -1.0]\nn = MLP(3, [4, 4, 1])\nn(x)\n\nValue(data=-0.8518646845522475)\n\n\n\nlen(n.parameters())\n\n41\n\n\n\nxs = [\n  [2.0, 3.0, -1.0],\n  [3.0, -1.0, 0.5],\n  [0.5, 1.0, 1.0],\n  [1.0, 1.0, -1.0],\n]\nys = [1.0, -1.0, -1.0, 1.0] # desired targets\n\n\nypred = [n(x) for x in xs]\nloss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\nloss\n\nValue(data=8.195423138300107)\n\n\n\nypred = [n(x) for x in xs]\nloss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\nloss\n\nValue(data=8.195423138300107)\n\n\n\nloss.backward()\n\n\nfor p in n.parameters():\n    p.data += -0.01 * p.grad\n\n\nypred\n\n[Value(data=-0.8518646845522475),\n Value(data=0.12883393385135233),\n Value(data=-0.5205839755359478),\n Value(data=-0.8060771173128451)]\n\n\n\nfor k in range(200):\n  \n  # forward pass\n  ypred = [n(x) for x in xs]\n  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n  \n  # backward pass\n  for p in n.parameters():\n    p.grad = 0.0\n  \n  loss.backward()\n  \n  # update\n  for p in n.parameters():\n    p.data += -0.1 * p.grad\n  \n  print(k, loss.data)\n\n0 7.9987601054344974\n1 7.998768254319929\n2 7.998776289198453\n3 7.998784212518286\n4 7.998792026656887\n5 7.998799733923519\n6 7.998807336561709\n7 7.99881483675159\n8 7.998822236612142\n9 7.9988295382033545\n10 7.998836743528274\n11 7.998843854534988\n12 7.99885087311851\n13 7.998857801122596\n14 7.9988646403414805\n15 7.998871392521546\n16 7.99887805936293\n17 7.998884642521051\n18 7.998891143608089\n19 7.998897564194406\n20 7.998903905809901\n21 7.998910169945322\n22 7.998916358053519\n23 7.998922471550658\n24 7.998928511817379\n25 7.998934480199918\n26 7.99894037801118\n27 7.998946206531777\n28 7.9989519670110205\n29 7.998957660667887\n30 7.998963288691943\n31 7.998968852244227\n32 7.998974352458117\n33 7.998979790440156\n34 7.998985167270847\n35 7.998990484005422\n36 7.998995741674587\n37 7.999000941285239\n38 7.999006083821148\n39 7.999011170243634\n40 7.9990162014922035\n41 7.999021178485171\n42 7.999026102120268\n43 7.999030973275209\n44 7.999035792808263\n45 7.999040561558786\n46 7.999045280347749\n47 7.999049949978248\n48 7.99905457123598\n49 7.999059144889733\n50 7.999063671691822\n51 7.999068152378555\n52 7.999072587670647\n53 7.999076978273633\n54 7.999081324878281\n55 7.9990856281609695\n56 7.99908988878407\n57 7.999094107396307\n58 7.9990982846331145\n59 7.999102421116978\n60 7.999106517457763\n61 7.999110574253039\n62 7.9991145920883895\n63 7.999118571537715\n64 7.999122513163526\n65 7.999126417517225\n66 7.999130285139385\n67 7.999134116560015\n68 7.999137912298817\n69 7.999141672865441\n70 7.999145398759733\n71 7.99914909047196\n72 7.999152748483052\n73 7.999156373264821\n74 7.999159965280181\n75 7.999163524983352\n76 7.99916705282007\n77 7.999170549227792\n78 7.999174014635873\n79 7.9991774494657735\n80 7.999180854131225\n81 7.999184229038425\n82 7.999187574586189\n83 7.999190891166139\n84 7.999194179162857\n85 7.99919743895404\n86 7.999200670910663\n87 7.999203875397127\n88 7.999207052771402\n89 7.999210203385175\n90 7.999213327583982\n91 7.999216425707352\n92 7.999219498088927\n93 7.999222545056597\n94 7.999225566932631\n95 7.999228564033784\n96 7.999231536671427\n97 7.9992344851516535\n98 7.9992374097753975\n99 7.999240310838541\n100 7.999243188632027\n101 7.999246043441947\n102 7.999248875549657\n103 7.999251685231876\n104 7.9992544727607715\n105 7.999257238404061\n106 7.999259982425107\n107 7.9992627050829945\n108 7.999265406632628\n109 7.99926808732482\n110 7.999270747406354\n111 7.99927338712009\n112 7.999276006705024\n113 7.999278606396377\n114 7.999281186425669\n115 7.99928374702078\n116 7.999286288406042\n117 7.999288810802287\n118 7.999291314426934\n119 7.999293799494042\n120 7.999296266214385\n121 7.999298714795511\n122 7.999301145441796\n123 7.999303558354529\n124 7.999305953731936\n125 7.99930833176927\n126 7.999310692658851\n127 7.999313036590123\n128 7.99931536374971\n129 7.999317674321473\n130 7.999319968486549\n131 7.999322246423416\n132 7.999324508307934\n133 7.999326754313392\n134 7.999328984610566\n135 7.999331199367747\n136 7.999333398750801\n137 7.999335582923215\n138 7.99933775204612\n139 7.999339906278356\n140 7.999342045776506\n141 7.9993441706949255\n142 7.999346281185799\n143 7.999348377399164\n144 7.99935045948296\n145 7.99935252758306\n146 7.999354581843308\n147 7.999356622405555\n148 7.999358649409688\n149 7.999360662993675\n150 7.999362663293588\n151 7.999364650443644\n152 7.999366624576229\n153 7.9993685858219346\n154 7.9993705343095876\n155 7.9993724701662785\n156 7.9993743935173915\n157 7.999376304486636\n158 7.999378203196072\n159 7.9993800897661345\n160 7.999381964315667\n161 7.9993838269619495\n162 7.999385677820711\n163 7.99938751700617\n164 7.999389344631057\n165 7.999391160806628\n166 7.999392965642702\n167 7.999394759247677\n168 7.999396541728552\n169 7.999398313190958\n170 7.999400073739171\n171 7.999401823476137\n172 7.999403562503496\n173 7.999405290921597\n174 7.999407008829529\n175 7.999408716325124\n176 7.999410413504997\n177 7.999412100464545\n178 7.999413777297985\n179 7.999415444098361\n180 7.999417100957562\n181 7.9994187479663506\n182 7.999420385214364\n183 7.999422012790147\n184 7.9994236307811555\n185 7.999425239273788\n186 7.99942683835339\n187 7.99942842810427\n188 7.999430008609728\n189 7.999431579952053\n190 7.999433142212552\n191 7.9994346954715585\n192 7.999436239808448\n193 7.999437775301655\n194 7.999439302028684\n195 7.999440820066123\n196 7.999442329489664\n197 7.999443830374101\n198 7.999445322793363\n199 7.99944680682051"
  },
  {
    "objectID": "01-workflows.html",
    "href": "01-workflows.html",
    "title": "workflows",
    "section": "",
    "text": "import torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n\n'2.1.0+cu121'\n\n\n\n# Create *known* parameters\nweight = 0.7\nbias = 0.3\n\n# Create data\nstart = 0\nend = 1\nstep = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1)\ny = weight * X + bias\n\nX[:10], y[:10]\n\n(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))\n\n\n\nX.shape, y.shape\n\n(torch.Size([50, 1]), torch.Size([50, 1]))\n\n\n\n# Create train/test split\ntrain_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n\n(40, 40, 10, 10)\n\n\n\ndef plot_predictions(train_data=X_train,\n                     train_labels=y_train,\n                     test_data=X_test,\n                     test_labels=y_test,\n                     predictions=None):\n  \"\"\"\n  Plots training data, test data and compares predictions.\n  \"\"\"\n  plt.figure(figsize=(10, 7))\n\n  # Plot training data in blue\n  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n\n  # Plot test data in green\n  plt.scatter(test_data, test_labels, c=\"orange\", s=4, label=\"Testing data\")\n\n  if predictions is not None:\n    # Plot the predictions in red (predictions were made on the test data)\n    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n  # Show the legend\n  plt.legend(prop={\"size\": 14});\n\n\nplot_predictions();\n\n\n\n\n\nclass Linear(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weights = nn.Parameter(torch.rand(1, requires_grad=True))\n        self.bias = nn.Parameter(torch.rand(1, requires_grad=True))\n\n    def forward(self, x):\n        return self.weights * x + self.bias\n\n\n# Set manual seed since nn.Parameter are randomly initialzied\ntorch.manual_seed(42)\n\n# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\n# model_0 = LinearModel()\nmodel_0 = Linear()\n\n# Check the nn.Parameter(s) within the nn.Module subclass we created\nlist(model_0.parameters())\n\n[Parameter containing:\n tensor([0.8823], requires_grad=True),\n Parameter containing:\n tensor([0.9150], requires_grad=True)]\n\n\n\n# List named parameters\nmodel_0.state_dict()\n\nOrderedDict([('weights', tensor([0.8823])), ('bias', tensor([0.9150]))])\n\n\n\nwith torch.inference_mode():\n    preds = model_0(X_test)\n\n\nplot_predictions(predictions=preds)\n\n\n\n\n\n# Create the loss function\nloss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n\n# Create the optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n                            lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))\n\n\ntorch.manual_seed(42)\n\n# Set the number of epochs (how many times the model will pass over the training data)\nepochs = 100\n\n# Create empty loss lists to track values\ntrain_loss_values = []\ntest_loss_values = []\nepoch_count = []\n\nfor epoch in range(epochs):\n    ### Training\n\n    # Put model in training mode (this is the default state of a model)\n    model_0.train()\n\n    # 1. Forward pass on train data using the forward() method inside\n    y_pred = model_0(X_train)\n    # print(y_pred)\n\n    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad of the optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Progress the optimizer\n    optimizer.step()\n\n    ### Testing\n\n    # Put the model in evaluation mode\n    model_0.eval()\n\n    with torch.inference_mode():\n      # 1. Forward pass on test data\n      test_pred = model_0(X_test)\n\n      # 2. Caculate loss on test data\n      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n\n      # Print out what's happening\n      if epoch % 10 == 0:\n            epoch_count.append(epoch)\n            train_loss_values.append(loss.detach().numpy())\n            test_loss_values.append(test_loss.detach().numpy())\n            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n\nEpoch: 0 | MAE Train Loss: 0.6860889196395874 | MAE Test Loss: 0.7637526988983154 \nEpoch: 10 | MAE Train Loss: 0.5708791017532349 | MAE Test Loss: 0.6290428042411804 \nEpoch: 20 | MAE Train Loss: 0.45566922426223755 | MAE Test Loss: 0.4943329691886902 \nEpoch: 30 | MAE Train Loss: 0.34045934677124023 | MAE Test Loss: 0.35962313413619995 \nEpoch: 40 | MAE Train Loss: 0.2252494841814041 | MAE Test Loss: 0.2249133139848709 \nEpoch: 50 | MAE Train Loss: 0.1100396141409874 | MAE Test Loss: 0.09020347893238068 \nEpoch: 60 | MAE Train Loss: 0.009724985808134079 | MAE Test Loss: 0.020998019725084305 \nEpoch: 70 | MAE Train Loss: 0.006216754671186209 | MAE Test Loss: 0.014099234715104103 \nEpoch: 80 | MAE Train Loss: 0.002788322512060404 | MAE Test Loss: 0.005826681852340698 \nEpoch: 90 | MAE Train Loss: 0.007095950655639172 | MAE Test Loss: 0.00754010071977973 \n\n\n\nnew_preds = model_0(X_test)\n\n\nnew_preds\n\ntensor([[0.8661],\n        [0.8801],\n        [0.8940],\n        [0.9080],\n        [0.9220],\n        [0.9359],\n        [0.9499],\n        [0.9638],\n        [0.9778],\n        [0.9917]], grad_fn=&lt;AddBackward0&gt;)\n\n\nYou canâ€™t plot a differentiable tensor.\n\ntry: plt.plot(new_preds)\nexcept Exception as e: print(e)\n\nCan't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n\n\n\n\n\n\np = new_preds.detach().numpy()\n\n\nplot_predictions(predictions=p)\n\n\n\n\n\nfrom pathlib import Path\n\n# 1. Create models directory\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path\nMODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict\nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH)\n\nSaving model to: models/01_pytorch_workflow_model_0.pth\n\n\n\ntorch.load(MODEL_SAVE_PATH)\n\nOrderedDict([('weights', tensor([0.6977])), ('bias', tensor([0.3080]))])\n\n\n\nsave_model = Linear()\nsave_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n\n&lt;All keys matched successfully&gt;\n\n\n\nsave_model(X_test)\n\ntensor([[0.8661],\n        [0.8801],\n        [0.8940],\n        [0.9080],\n        [0.9220],\n        [0.9359],\n        [0.9499],\n        [0.9638],\n        [0.9778],\n        [0.9917]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nX_test.shape\n\ntorch.Size([10, 1])\n\n\nYou can practically use any shape tensor for prediction as in the forward pass, you only multiply it with a scalar and then add one. Due to operator broadcasting, you wonâ€™t face any shape mismatch errors.\n\nsave_model(torch.rand((10, 9)))\n\ntensor([[0.7863, 0.9844, 0.6076, 0.6541, 0.5765, 0.3656, 0.8242, 0.3105, 0.8734],\n        [0.9178, 0.9867, 0.5746, 0.3702, 0.7353, 0.8495, 0.3096, 0.5777, 0.4477],\n        [0.6263, 0.4851, 0.5142, 0.5461, 0.3253, 0.9431, 0.9493, 0.6021, 0.6171],\n        [0.5145, 0.3418, 0.3174, 0.7865, 0.4653, 0.4326, 0.6296, 0.5407, 0.5440],\n        [0.6680, 0.5828, 0.5367, 0.4898, 0.3729, 0.9493, 0.5172, 0.7493, 0.5358],\n        [0.6852, 0.9821, 0.8176, 0.3545, 0.7953, 0.9880, 0.7486, 0.8907, 1.0008],\n        [0.6034, 0.7292, 0.4144, 0.5850, 0.9152, 0.8357, 0.4361, 0.3771, 0.4185],\n        [0.3126, 0.3877, 0.5706, 0.8923, 0.7152, 0.3915, 0.3770, 0.8304, 0.3974],\n        [0.6139, 0.8242, 0.4954, 0.6188, 0.6265, 0.5743, 0.4800, 0.3459, 0.3749],\n        [0.4701, 0.9938, 0.4883, 0.4226, 0.7414, 0.7530, 0.8480, 0.9220, 0.8511]],\n       grad_fn=&lt;AddBackward0&gt;)"
  }
]